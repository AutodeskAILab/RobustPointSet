{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import shutil\n",
    "import importlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from data_utils.ModelNetDataLoader import ModelNetDataLoader\n",
    "from data_utils import provider\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'log'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, num_class=40):\n",
    "    mean_correct = []\n",
    "    class_acc = np.zeros((num_class,3))\n",
    "    for j, data in tqdm(enumerate(loader), total=len(loader)):\n",
    "        points, target = data\n",
    "        target = target[:, 0]\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.cuda(), target.cuda()\n",
    "        classifier = model.eval()\n",
    "        pred, _ = classifier(points.float())\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        for cat in np.unique(target.cpu()):\n",
    "            cat = int(cat)\n",
    "            classacc = pred_choice[target==cat].eq(target[target==cat].long().data).cpu().sum()\n",
    "            class_acc[cat,0]+= classacc.item()/float(points[target==cat].size()[0])\n",
    "            class_acc[cat,1]+=1\n",
    "        correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "        mean_correct.append(correct.item()/float(points.size()[0]))\n",
    "    class_acc[:,2] =  class_acc[:,0]/ class_acc[:,1]\n",
    "    class_acc = np.mean(class_acc[:,2])\n",
    "    instance_acc = np.mean(mean_correct)\n",
    "    return instance_acc, class_acc\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "        \n",
    "    ### Hyper Parameters ###\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    \n",
    "    ### Create Dir ###\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))+'-'+args.model+'-'+args.task\n",
    "    experiment_dir = Path('log/')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    experiment_dir = experiment_dir.joinpath('classification')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    if args.log_dir is None:\n",
    "        experiment_dir = experiment_dir.joinpath(timestr)\n",
    "    else:\n",
    "        experiment_dir = experiment_dir.joinpath(args.log_dir)\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = experiment_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    ### Log ###\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)\n",
    "    \n",
    "    \n",
    "    ### Data Loading ###\n",
    "    log_string('Load dataset ...')\n",
    "    TRAIN_DATASET = ModelNetDataLoader(root=args.data_root, \n",
    "                                   tasks=args.train_tasks,\n",
    "                                   labels=args.train_labels,\n",
    "                                   partition='train',\n",
    "                                   npoint=args.num_point,      \n",
    "                                   normal_channel=args.normal)\n",
    "    TEST_DATASET = ModelNetDataLoader(root=args.data_root, \n",
    "                                   tasks=args.test_tasks,\n",
    "                                   labels=args.test_labels,\n",
    "                                   partition='test',\n",
    "                                   npoint=args.num_point,      \n",
    "                                   normal_channel=args.normal)\n",
    "    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    \n",
    "    ### Model Loading ###\n",
    "    num_class = 40\n",
    "    MODEL = importlib.import_module(args.model)\n",
    "    shutil.copy('models/%s.py' % args.model, str(experiment_dir))\n",
    "    # pointnet_util pointnet_util, \n",
    "    shutil.copy('models/%s.py' % args.ults, str(experiment_dir))\n",
    "    \n",
    "    classifier = MODEL.get_model(num_class,normal_channel=args.normal).cuda()\n",
    "    criterion = MODEL.get_loss().cuda()\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        log_string('Use pretrain model')\n",
    "    except:\n",
    "        log_string('No existing model, starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "        \n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), \n",
    "                                    lr=0.01, \n",
    "                                    momentum=0.9)\n",
    "\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step, gamma=args.lr_decay_rate)\n",
    "    \n",
    "    global_epoch = 0\n",
    "    global_step = 0\n",
    "    best_instance_acc = 0.0\n",
    "    best_class_acc = 0.0\n",
    "    mean_correct = []\n",
    "    \n",
    "    \n",
    "    ### Training ###\n",
    "    logger.info('Start training ...')\n",
    "    for epoch in range(start_epoch, args.epoch): \n",
    "        \n",
    "        \n",
    "        log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        \n",
    "        #scheduler.step()\n",
    "        for batch_id, data in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n",
    "            points, target = data\n",
    "            \n",
    "            points = points.data.numpy()\n",
    "            \n",
    "            \"\"\"\n",
    "            # augmentation is not used here\n",
    "            points = provider.random_point_dropout(points)\n",
    "            points[:,:, 0:3] = provider.random_scale_point_cloud(points[:,:, 0:3])\n",
    "            points[:,:, 0:3] = provider.shift_point_cloud(points[:,:, 0:3])\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "            points = torch.Tensor(points)\n",
    "            target = target[:, 0]\n",
    "\n",
    "            points = points.transpose(2, 1)\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classifier = classifier.train()\n",
    "            pred, trans_feat = classifier(points)\n",
    "            loss = criterion(pred, target.long(), trans_feat)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "            mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        \n",
    "        train_instance_acc = np.mean(mean_correct)\n",
    "        log_string('Train Instance Accuracy: %f' % train_instance_acc)\n",
    "        \n",
    "    \n",
    "        if (epoch % 2 == 0) or (epoch == args.epoch):\n",
    "            with torch.no_grad():\n",
    "                instance_acc, class_acc = test(classifier.eval(), testDataLoader)\n",
    "\n",
    "                if (instance_acc >= best_instance_acc):\n",
    "                    best_instance_acc = instance_acc\n",
    "                    best_epoch = epoch + 1\n",
    "\n",
    "                if (class_acc >= best_class_acc):\n",
    "                    best_class_acc = class_acc\n",
    "                log_string('Test Instance Accuracy: %f, Class Accuracy: %f'% (instance_acc, class_acc))\n",
    "                log_string('Best Instance Accuracy: %f, Class Accuracy: %f'% (best_instance_acc, best_class_acc))\n",
    "\n",
    "                if (instance_acc >= best_instance_acc):\n",
    "                    logger.info('Save model...')\n",
    "                    savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "                    log_string('Saving at %s'% savepath)\n",
    "                    state = {\n",
    "                        'epoch': best_epoch,\n",
    "                        'instance_acc': instance_acc,\n",
    "                        'class_acc': class_acc,\n",
    "                        'model_state_dict': classifier.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }\n",
    "                    torch.save(state, savepath)\n",
    "        global_epoch += 1\n",
    "        \n",
    "        # adjust lr\n",
    "        scheduler.step()\n",
    "        #for param_group in optimizer.param_groups:\n",
    "        #    print('lr: ' + str(param_group['lr']))\n",
    "        \n",
    "    logger.info('End of training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # traininig hyper-parameters\n",
    "    batch_size = 32 \n",
    "    model ='pointnet_cls'  \n",
    "    ults = 'pointnet_util' \n",
    "    task = 's1'         # Strategy 1 or 2\n",
    "    epoch = 300\n",
    "    learning_rate = 0.001\n",
    "    gpu = '0' \n",
    "    num_point = 2048\n",
    "    optimizer = 'Adam'\n",
    "    log_dir = None\n",
    "    decay_rate = 1e-4\n",
    "    lr_decay_rate = 0.5\n",
    "    lr_decay_step = 100\n",
    "    lr_clip = 1e-7\n",
    "    normal = False # wheather to use normal\n",
    "    data_root = 'data/'\n",
    "    \n",
    "    # Available data: original, jitter, translate, missing_part, sparse, rotation, occlusion\n",
    "    \n",
    "    # An example for Strategy 1: training-domain validation\n",
    "    train_tasks = ['train_original.npy']                    # define which data you want to train on\n",
    "    test_tasks = ['test_original.npy']                      # define which data you want to test on\n",
    "    train_labels = ['train_labels.npy']*len(train_tasks)  # the label will be the same for each augmentation\n",
    "    test_labels =  ['test_labels.npy']*len(test_tasks)\n",
    "    \n",
    "    \"\"\"\n",
    "    # An example for Strategy 2: leave-one-out validation\n",
    "    train_tasks = ['train_original.npy', 'train_jitter.npy', 'train_translate.npy', 'train_missing_part.npy', 'train_sparse.npy', 'train_rotation.npy']                  \n",
    "    test_tasks = ['test_occlusion.npy']                    \n",
    "    train_labels = ['train_labels.npy']*len(train_tasks)  \n",
    "    test_labels =  ['test_labels.npy']*len(test_tasks)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = Args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
